<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Kuang-Huei Lee | Personal Website</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!-- Google Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-117349293-1', 'auto');
    ga('send', 'pageview');
    </script>
    <!-- End Google Analytics -->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <img src="./docs/images/profile.jpg" width="160"><br><br>
        <h1>Kuang-Huei Lee</h1>
        <p class="view">
          <a href="https://scholar.google.com/citations?user=rE7-N30AAAAJ">[Google Scholar]</a>
          <a href="https://www.linkedin.com/in/kuanghuei">[LinkedIn]</a>
          <a href="https://github.com/kuanghuei">[GitHub]</a>
        </p>
        <p>kuanghul AT alumni.cmu.edu</p>
      </header>

      <section>
        <p>Kuang-Huei Lee is a Staff Research Scientist at Google DeepMind in San Francisco. His research interests center around creating general cognitive agents in both physical and virtual worlds, and his current research spans deep generative models, reasoning, planning, reinforcement learning, and robotics. Prior to joining Google in 2019, Kuang-Huei spent 3 years at Microsoft. He received his graduate degree in Computer Science from Carnegie Mellon University, and his undergraduate degree in Mechanical Engineering from National Taiwan University. His research has been widely published, appearing in venues such as NeurIPS, ICML, ICLR, CoRL, RSS, IROS, CVPR, ECCV, and EMNLP.</p>
      </section>

      <section>
        <h2>
          <a id="Publications" class="anchor" href="#Publications" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Selected Publications (<a href="https://scholar.google.com/citations?user=rE7-N30AAAAJ">All Publications</a>)
        </h2>

        <p>
          <font color="black">
            <em>Evolving Deeper LLM Thinking.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>*&#8224;, Ian Fischer*, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, Xinyun Chen
            <br>(*: first author, &#8224;: senior author)
            <br> 
            arXiv.
            <br> 
            [<a href="https://arxiv.org/abs/2501.09891">paper</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Training-free Diffusion Model Alignment with Sampling Demons.</em>
          </font>
          <br>
          
          <font size="2">
            Po-Hung Yeh, <b>Kuang-Huei Lee</b>, Jun-Cheng Chen.
            <br> 
            ICLR 2025.
            <br> 
            [<a href="https://arxiv.org/abs/2410.05760">paper</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Geometric-Averaged Preference Optimization for Soft Preference Labels.</em>
          </font>
          <br>
          
          <font size="2">
            Hiroki Furuta, <b>Kuang-Huei Lee</b>, Shixiang Shane Gu, Yutaka Matsuo, Aleksandra Faust, Heiga Zen, Izzeddin Gur.
            <br> 
            NeurIPS 2024.
            <br> 
            [<a href="https://arxiv.org/abs/2409.06691">paper</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>The Design of the Barkour Benchmark for Robot Agility.</em>
          </font>
          <br>
          
          <font size="2">
            44 authors including <b>Kuang-Huei Lee</b> (last author).
            <br> 
            IROS 2024.
          </font>
        </p>

        <p>
          <font color="black">
            <em>A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer.
            <br> 
            ICML 2024.
            <br> 
            [<a href="https://arxiv.org/abs/2402.09727">paper</a>]
            [<a href="https://read-agent.github.io">website and demo</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs.</em>
          </font>
          <br>
          
          <font size="2">
            Soroush Nasiriany*, Fei Xia*, Wenhao Yu*, Ted Xiao*, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, <b>Kuang-Huei Lee</b>, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, Brian Ichter*
            <br> 
            ICML 2024.
            <br> 
            [<a href="https://arxiv.org/abs/2402.07872">paper</a>]
            [<a href="https://pivot-prompt.github.io">website and demo</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Learning to Learn Faster from Human Feedback with Language Model Predictive Control.</em>
          </font>
          <br>
          
          <font size="2">
            Jacky Liang*, Fei Xia*, Wenhao Yu*, Andy Zeng*, and 46 others including <b>Kuang-Huei Lee</b>.
            <br> 
            RSS 2024.
            <br> 
            [<a href="https://arxiv.org/abs/2402.11450">paper</a>]
            [<a href="https://robot-teaching.github.io">website and demo</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Multimodal Web Navigation with Instruction-Finetuned Foundation Models.</em>
          </font>
          <br>
          
          <font size="2">
            Hiroki Furuta, <b>Kuang-Huei Lee</b>, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, Izzeddin Gur.
            <br> 
            ICLR 2024.
            <br> 
            [<a href="https://arxiv.org/abs/2305.11854">paper</a>]
            [<a href="https://sites.google.com/view/mm-webnav/">website</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Language to Rewards for Robotic Skill Synthesis.</em>
          </font>
          <br>
          
          <font size="2">
            Wenhao Yu*, Nimrod Gileadi*, Chuyuan Fu&#8224;, Sean Kirmani&#8224;, <b>Kuang-Huei Lee&#8224;</b>, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, Fei Xia.
            <br>(*: Co-first authors, equal contribution, &#8224;: Core contributors)
            <br> 
            CoRL 2023 <b>Oral presentation</b>.
            <br> 
            [<a href="https://arxiv.org/abs/2306.08647">paper</a>]
            [<a href="https://language-to-reward.github.io">website</a>]
            [<a href="https://ai.googleblog.com/2023/08/language-to-rewards-for-robotic-skill.html">Google AI Blog</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Barkour: Benchmarking Animal-level Agility with Quadruped Robots.</em>
          </font>
          <br>
           
          <font size="2">
            Ken Caluwaerts*, Atil Iscen*, J. Chase Kew*, Wenhao Yu*, Tingnan Zhang*, Daniel Freeman&#8224;, <b>Kuang-Huei Lee&#8224;</b>, Lisa Lee&#8224;, Stefano Saliceti&#8224;, Vincent Zhuang&#8224;, ..., (31 others), ..., Vincent Vanhoucke, and Jie Tan.
            <br>(*: Co-first authors, equal contribution, &#8224;: Core contributors)
            <br> 
            Tech Report.
            <br> 
            [<a href="https://arxiv.org/abs/2305.14654">paper</a>]
            [<a href="https://sites.google.com/corp/view/barkour">website</a>]
            [<a href="https://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html">Google AI Blog</a>]
            [<a href="https://www.youtube.com/watch?v=EcYzl_qWLKg">YouTube</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators.</em>
          </font>
          <br>
          
          <font size="2">
            {Alexander Herzog, Kanishka Rao, Karol Hausman, Yao Lu, Paul Wohlhart} and 35 others including <b>Kuang-Huei Lee</b>.
            <br> 
            RSS 2023 <b>Oral presentation</b>.
            <br> 
            [<a href="https://arxiv.org/abs/2305.03270">paper</a>]
            [<a href="https://rl-at-scale.github.io">website</a>]
            [<a href="https://ai.googleblog.com/2023/04/robotic-deep-rl-at-scale-sorting-waste.html">Google AI Blog</a>]
            [<a href="https://youtu.be/CHS9HSb1uqA">YouTube</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>RT-1: Robotics Transformer for Real-World Control at Scale.</em>
          </font>
          <br>
          
          <font size="2">
            51 authors in alphabetical order including <b>Kuang-Huei Lee</b>.
            <br> 
            RSS 2023 <b>Oral presentation</b>.
            <br> 
            [<a href="https://arxiv.org/abs/2212.06817">paper</a>]
            [<a href="https://robotics-transformer.github.io">website</a>]
            [<a href="https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">Google AI Blog</a>]
            [<a href="https://youtu.be/UuKAp9a6wMs">YouTube</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.</em>
          </font>
          <br>
          
          <font size="2">
            43 authors in alphabetical order including <b>Kuang-Huei Lee</b>.
            <br> 
            CoRL 2022 <b>Special Innovation Award</b>.
            <br> 
            [<a href="https://arxiv.org/abs/2204.01691">paper</a>]
            [<a href="https://say-can.github.io">website</a>]
            [<a href="https://ai.googleblog.com/2022/08/towards-helpful-robots-grounding.html">Google AI Blog</a>]
            [<a href="https://www.youtube.com/watch?v=E2g1APtSuUM">YouTube</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>PI-QT-Opt: Predictive Information Improves Multi-Task Robotic Reinforcement Learning at Scale.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>*, Ted Xiao, Adrian Li, Paul Wohlhart, Ian Fischer, Yao Lu*
            <br> 
            CoRL 2022.
            <br> 
            [<a href="https://arxiv.org/abs/2210.08217">paper</a>]
            [<a href="http://piqtopt.github.io/">website</a>]
            [<a href="https://kuanghuei.github.io/piqtopt">supplementary video</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Multi-Game Decision Transformers.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>*, Ofir Nachum*, Mengjiao Yang, Lisa Lee, Daniel Freeman, Winnie Xu, Sergio Guadarrama, Ian Fischer, Eric Jang, Henryk Michalewski, Igor Mordatch*
            <br>(*: Equal contribution)
            <br> 
            NeurIPS 2022 <b>Oral presentation</b>.
            <br> 
            [<a href="https://arxiv.org/abs/2205.15241">paper</a>]
            [<a href="https://sites.google.com/view/multi-game-transformers">website</a>]
            [<a href="https://github.com/google-research/google-research/tree/master/multi_game_dt">code and pre-trained model</a>]
            [<a href="https://ai.googleblog.com/2022/07/training-generalist-agents-with-multi.html">Google AI Blog</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Deep Hierarchical Planning from Pixels.</em>
          </font>
          <br>
          
          <font size="2">
            Danijar Hafner, <b>Kuang-Huei Lee</b>, Ian Fischer, Pieter Abbeel
            <br> 
            NeurIPS 2022.
            <br> 
            [<a href="https://arxiv.org/abs/2206.04114">paper</a>]
            [<a href="https://danijar.com/project/director/">website</a>]
            [<a href="https://ai.googleblog.com/2022/07/deep-hierarchical-planning-from-pixels.html">Google AI Blog</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>*, Ofir Nachum*, Tingnan Zhang, Sergio Guadarrama, Jie Tan, Wenhao Yu
            <br>(*: Equal contribution)
            <br> 
            IROS 2022 <b>Best Paper Finalist</b>.
            <br> 
            [<a href="https://arxiv.org/abs/2207.13224">paper</a>]
            [<a href="https://kuanghuei.github.io/piars">supplementary video</a>]
            [<a href="https://ai.googleblog.com/2022/10/pi-ars-accelerating-evolution-learned.html">Google AI Blog</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Compressive Visual Representations.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>&#8224;, 
            <a href="https://scholar.google.com/citations?user=l2FS2_IAAAAJ">Anurag Arnab</a>&#8224;,
            <a href="https://scholar.google.com/citations?user=gYiCq88AAAAJ">Sergio Guadarrama</a>,
            <a href="https://scholar.google.com/citations?user=LAv0HTEAAAAJ">John Canny</a>,
            <a href="https://scholar.google.com/citations?user=Z63Zf_0AAAAJ">Ian Fischer</a>&#8224;.
            <br>(&#8224;: Main contributors)
            <br> 
            NeurIPS 2021.
            <br> 
            [<a href="https://arxiv.org/abs/2109.12909">paper</a>]
            [<a href="https://github.com/google-research/compressive-visual-representations">code and pre-trained models</a>]
          </font>
        </p>

        <!-- <p>
          <font color="black">
            <em>An Empirical Investigation of Representation Learning for Imitation.</em>
          </font>
          <br>
          
          <font size="2">
            Xin Chen*, Sam Toyer*, Cody Wild*, Scott Emmons, Ian Fischer, <b>Kuang-Huei Lee</b>, Neel Alex, Steven H Wang, Ping Luo, Stuart Russell, Pieter Abbeel, Rohin Shah.
            <br> 
            NeurIPS 2021.
            [<a href="https://openreview.net/forum?id=kBNhgqXatI">paper</a>]
            [<a href="https://github.com/HumanCompatibleAI/eirli">code</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Learning Task Sampling Policy for Multitask Learning.</em>
          </font>
          <br>
          
          <font size="2">
            Dhanasekar Sundararaman, Henry Tsai, <b>Kuang-Huei Lee</b>, Iulia Turc, Lawrence Carin.
            <br> 
            Findings of EMNLP 2021.
            [<a href="https://research.google/pubs/pub50815/">paper</a>]
          </font>
        </p> -->

        <p>
          <font color="black">
            <em>Predictive Information Accelerates Learning in RL.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>, 
            <a href="https://scholar.google.com/citations?user=Z63Zf_0AAAAJ">Ian Fischer</a>,
            <a href="https://scholar.google.com/citations?user=TjEqCOAAAAAJ">Anthony Liu</a>,
            <a href="https://scholar.google.com/citations?user=ONuIPv0AAAAJ">Yijie Guo</a>,
            <a href="https://scholar.google.com/citations?user=fmSHtE8AAAAJ">Honglak Lee</a>,
            <a href="https://scholar.google.com/citations?user=LAv0HTEAAAAJ">John Canny</a>,
            <a href="https://scholar.google.com/citations?user=gYiCq88AAAAJ">Sergio Guadarrama</a>.
            <br> 
            NeurIPS 2020.
            <br> 
            [<a href="https://arxiv.org/abs/2007.12401">paper</a>]
            [<a href="https://github.com/google-research/pisac">code</a>]
          </font>
        </p>

        <!-- <p>
          <font color="black">
            <em>Towards Scalable Image Classifier Learning with Noisy Labels via Domain Adaptation.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>, 
            <a href="https://scholar.google.com/citations?user=W5WbqgoAAAAJ">Xiaodong He</a>,
            <a href="https://scholar.google.com/citations?user=cvgKxDQAAAAJ">Linjun Yang</a>,
            <a href="https://scholar.google.com/citations?user=fIlGZToAAAAJ">Lei Zhang</a>.
            <br> 
            A book chapter in "Domain Adaptation in Computer Vision with Deep Learning".
            [<a href="https://link.springer.com/chapter/10.1007/978-3-030-45529-3_9">link</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Learning Visual Relation Priors for Image-Text Matching and Image Captioning with Neural Scene Graph Generators.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee*</b>,
    			  Hamid Palangi*,
            <a href="https://sites.google.com/site/xichenstephen/">Xi Chen</a>, 
            Houdong Hu, 
            <a href="https://scholar.google.com/citations?user=CQ1cqKkAAAAJ">Jianfeng Gao</a>.
            <br> 
            Tech report. (*: Equal contributions)
            [<a href="https://arxiv.org/abs/1909.09953">paper</a>]
          </font>
        </p> -->
	  
        <p>
          <font color="black">
            <em>Stacked Cross Attention for Image-Text Matching.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>, 
            <a href="https://sites.google.com/site/xichenstephen/">Xi Chen</a>, 
            <a href="http://www.ganghua.org">Gang Hua</a>, 
            Houdong Hu, 
            <a href="https://scholar.google.com/citations?user=W5WbqgoAAAAJ">Xiaodong He</a>.
            <br> 
            ECCV 2018.
            <br> 
            [<a href="https://arxiv.org/abs/1803.08024">paper</a>]
            [<a href="https://kuanghuei.github.io/SCANProject/">website</a>]
            [<a href="https://github.com/kuanghuei/SCAN">code</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>CleanNet: Transfer Learning for Scalable Image Classifier Training with Label Noise.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>, 
            <a href="https://scholar.google.com/citations?user=W5WbqgoAAAAJ">Xiaodong He</a>, 
            <a href="https://scholar.google.com/citations?user=fIlGZToAAAAJ">Lei Zhang</a>, 
            <a href="https://scholar.google.com/citations?user=cvgKxDQAAAAJ">Linjun Yang</a>.
            <br> 
            CVPR 2018.
            <br>
            [<a href="https://arxiv.org/abs/1711.07131">paper</a>]
            [<a href="https://kuanghuei.github.io/CleanNetProject/">website</a>]
            [<a href="https://github.com/kuanghuei/clean-net">code</a>]
            [<a href="https://kuanghuei.github.io/Food-101N">dataset</a>]
            [<a href="https://www.microsoft.com/en-us/research/blog/using-transfer-learning-to-address-label-noise-for-large-scale-image-classification">MSR Blog</a>]
            [<a href="https://blogs.bing.com/search-quality-insights/2018-06/Artificial-intelligence-human-intelligence-Training-data-breakthrough">Bing Blog</a>]
          </font>
        </p>

        <!-- <p>
          <font color="black">
            <em>Computational modeling of effects of intravascular stent design on key mechanical and hemodynamic behavior.</em>
          </font>
          <br>
          
          <font size="2">
            Hao-Ming Hsiao, Yi-Hsiang Chiu, <b>Kuang-Huei Lee</b>, Chien-Han Lin.
            <br> Computer-Aided Design 2012
            [<a href="https://dl.acm.org/doi/abs/10.1016/j.cad.2012.03.009">paper</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Cardiovascular stent design and wall shear stress distribution in coronary stented arteries.</em>
          </font>
          <br>
          
          <font size="2">
            Hao-Ming Hsiao, <b>Kuang-Huei Lee</b>, Ying-Chih Liao, Yu-Chen Cheng.
            <br> Micro & Nano Letters 2012.
            [<a href="https://www.researchgate.net/publication/259948570_Cardiovascular_stent_design_and_wall_shear_stress_distribution_in_coronary_stented_arteries">paper</a>]
          </font>
        </p>

        <p>
          <font color="black">
            <em>Development of Computational Models for Evaluation of Mechanical and Hemodynamic Behavior of an Intravascular Stent.</em>
          </font>
          <br>
          
          <font size="2">
            <b>Kuang-Huei Lee</b>, Hao-Ming Hsiao, Ying-Chih Liao, Yi-Hsiang Chiu, Yu-Seng Tee.
            <br> ASME 6th Frontiers in Biomedical Devices Conference 2011. (oral)
            [<a href="https://asmedigitalcollection.asme.org/BIOMED/proceedings-abstract/BIOMED2011/54709/5/227309">paper</a>]
          </font>
        </p> -->
      </section>
      <section>
        <h2>
          <a id="Datasets" class="anchor" href="#Datasets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software
        </h2>
        <p>
          <font color="black">
            <em>TF-Agents. </em> [<a href="https://github.com/tensorflow/agents">GitHub</a>]
            <br>
            I am a main contributor of the TF-Agents RL library.
          </font>
        </p>
      </section>
      <section>
        <h2>
          <a id="Datasets" class="anchor" href="#Datasets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Datasets
        </h2>
        <p>
          <font color="black">
            <em>Food-101N: a dataset for learning to address label noise. </em>
            [<a href="https://kuanghuei.github.io/Food-101N">dataset</a>]
          </font>
        </p>
      </section>
      <section>
        <p><font size="1">
          Last Update: October 2024<br>
          Copyright 2024 Kuang-Huei Lee<br>            
        </font></p>
      </section>
    </div>
  </body>
</html>
